###############################################################################
#                               General Details                               #
###############################################################################
#                                                                             #
# Name                                                                        #
#     : default.env.properties                                                #
#                                                                             #
# Description                                                                 #
#     : This file contains properties which would be common or can be used    #
#       across subject areas and respective projects                          #
# Author                                                                      #
#     : 	                                                              #
#                                                                             #
###############################################################################
#                     Global Environment Base Properties                      #
###############################################################################

###############################################################################
#                                 Error Codes                                 #
###############################################################################

###
# These codes define the state of a record at any given state of the proc
# esing also these code would be used to partition all the processing tab
# les and also help in the whole tracking of each records all the way fro
# m incoming table till gold.
# @type  : String
# @Final : true
ERROR_CODE_INITIAL="10"
ERROR_CODE_PROCESSING="30"
ERROR_CODE_PROCESSED="70"
ERROR_CODE_RECONCILE="41"
ERROR_CODE_ERROR="90"


###############################################################################
#                                 Data Layers                                 #
###############################################################################

###                                                                           
# Stores data which is very sensitive as per government regulations eg PH     
# I PII After data is copied here it is various masking or encryption tec     
# hniques can be applied on it before making it available to users in inc     
# oming layer for further processing Extreme caution should be taken whil     
# e providing access to this layer Data from other layers can be moved in     
# to this layer for unmasking or decryption Define all HDFS paths for sec     
# urity data using this property only                                         
# @Type  :  Path
# @Final :  true
DATA_LAYER_DIR_SECURITY=${HDFS_PREFIX}/security

###
# Stores intermediate outputs which can be shared across multiple jobs fo
# r further processing Jobs creating the intermediate output may delete t
# his before or after execution Those policies differ job to job But ther
# e should be a corporate policy that cleans up data that has been lying
# in this layer for more than configurable amount of time Define all HDFS
# paths for stage data using this property only
# @Type  :  Path
# @Final :  true
DATA_LAYER_DIR_STAGING=${HDFS_PREFIX}/staging





###                                                                           
# Stores raw data which is not sensitive as per government regulations eg     
# PHI PII Data from various source systems lands into this layer It coul      
# d also be used for data discovery Only privileged users should be give      
# access to this data Data stored in this layer is partitioned based on t     
# he source system subject area It stores all the historical data It can      
# be used to recreate all the analytical views in case of disaster To do      
# that all data from this layer should be backed up time to time so separ     
# ate physical location Define all HDFS paths for incoming data using thi     
# s property only                                                             
# @Type  :  Path
# @Final :  true
DATA_LAYER_DIR_INCOMING=${HDFS_PREFIX}/incoming

###                                                                           
# Stores raw data which is not sensitive as per government regulations eg     
# PHI PII Data from various source systems lands into this layer It coul      
# d also be used for data discovery Only privileged users should be give      
# access to this data Data stored in this layer is partitioned based on t     
# he source system subject area It stores all the historical data It can      
# be used to recreate all the analytical views in case of disaster To do      
# that all data from this layer should be backed up time to time so separ     
# ate physical location Define all HDFS paths for incoming data using thi     
# s property only. This directory contains data ingested by hadoop ecosystem
# components.                                                             
# @Type  :  Path
# @Final :  true
DATA_LAYER_DIR_INCOMING_RAW=${DATA_LAYER_DIR_INCOMING}/raw


###                                                                           
# Stores intermediate outputs which can be shared across multiple jobs fo     
# r further processing Jobs creating the intermediate output may delete t     
# his before or after execution Those policies differ job to job But ther     
# e should be a corporate policy that cleans up data that has been lying      
# in this layer for more than configurable amount of time Define all HDFS     
# paths for work data using this property only                                
# @Type  :  Path
# @Final :  true
DATA_LAYER_DIR_WORK=${HDFS_PREFIX}/work


###                                                                           
# Stores data in denormalized format which is enriched integrated validat     
# ed from various sources The data stored in this layer is expected to be     
# available for entire organization as single source of truth Define all      
# HDFS paths for gold data using this property only                           
# @Type  :  Path
# @Final :  true
DATA_LAYER_DIR_GOLD=${HDFS_PREFIX}/gold


###                                                                           
# Stores use case specific data in denormalized format which is enriched      
# integrated validated from various sources Use cases might related to a      
# particular report or algorithm which in not to be used by anyone else D     
# efine all HDFS paths for smith data using this property only                
# @Type  :  Path
# @Final :  true
DATA_LAYER_DIR_SMITH=${HDFS_PREFIX}/smith


###                                                                           
# Stores data that needs to be archived For example Email backups Databas     
# e backups etc This layer is not for analytical purpose but purely for a     
# rchive purpose Define all HDFS paths for archive data using this proper     
# ty only                                                                     
# @Type  :  Path
# @Final :  true
DATA_LAYER_DIR_ARCHIVE=${HDFS_PREFIX}/archive


###                                                                           
# Stores data in the format expected by the downstream systems Even thoug     
# h gold or smith layers may contain denormalized views in certain format     
# downstream systems eg Teradata MainFrame may expect data in particular      
# format For example MainFrame would need data in EBCIDIC format but the      
# gold and smith tables are storing data in ORC or Parquet format Define      
# all HDFS paths for outgoing data using this property only                   
# @Type  :  Path
# @Final :  true
DATA_LAYER_DIR_OUTGOING=${HDFS_PREFIX}/outgoing


###                                                                           
# Stores backup of various layer explained above Policy should be designe     
# d in such a way that in case of disaster entire analytical data landsca     
# pe can be easily created from scratch This layer shall be on separate p     
# hysical layer Define all HDFS paths for backup data using this property     
# only                                                                        
# @Type  :  Path
# @Final :  true
DATA_LAYER_DIR_BACKUP=${HDFS_PREFIX}/backup

###                                                                           
# Stores sequence for coming records. This is required to generate surrogate      
# key for records.     
# @Type  :  Path
# @Final :  true
DATA_LAYER_DIR_SEQ=${HDFS_PREFIX}/sequence

###############################################################################
#                             variables for iitial load                       #
###############################################################################
###
# Variables needed for initial load
DATA_LAYER_DIR_INCOMING_RAW_INITIAL_LOAD=${DATA_LAYER_DIR_INCOMING}/raw/initial_load


###############################################################################
#                         Hive environment properties                         #
###############################################################################

###
# Points to security layer that is defined in Data Layers section above
# @Type  :  String
# @Final :  true
HIVE_DATABASE_NAME_STAGING=staging



###                                                                           
# Points to security layer that is defined in Data Layers section above       
# @Type  :  String
# @Final :  true
HIVE_DATABASE_NAME_SECURITY=security


###                                                                           
# Points to incoming layer that is defined in Data Layers section above       
# @Type  :  String
# @Final :  true
HIVE_DATABASE_NAME_INCOMING=incoming


###                                                                           
# Points to work layer that is defined in Data Layers section above           
# @Type  :  String
# @Final :  true
HIVE_DATABASE_NAME_WORK=work


###                                                                           
# Points to gold layer that is defined in Data Layers section above           
# @Type  :  String
# @Final :  true
HIVE_DATABASE_NAME_GOLD=gold


###                                                                           
# Points to smith layer that is defined in Data Layers section above          
# @Type  :  String
# @Final :  true
HIVE_DATABASE_NAME_SMITH=smith


###                                                                           
# Points to archive layer that is defined in Data Layers section above        
# @Type  :  String
# @Final :  true
HIVE_DATABASE_NAME_ARCHIVE=archive


###                                                                           
# Points to outgoing layer that is defined in Data Layers section above       
# @Type  :  String
# @Final :  true
HIVE_DATABASE_NAME_OUTGOING=outgoing


###                                                                           
# Points to backup layer that is defined in Data Layers section above         
# @Type  :  String
# @Final :  true
HIVE_DATABASE_NAME_BACKUP=backup

###
# Points to source express datahub 
# @Type  :  String
# @Final :  true
HIVE_DATABASE_NAME_CDH=cmhpcdw


###############################################################################
#                                     Log                                     #
###############################################################################

###                                                                           
# Root directory to store all logs based on subject area project module a     
# nd batch Admin shall implement appropriate policies to clean the logs o     
# r upload them to HDFS after those logs have been present for more than      
# configured amount of time Make use of functions defined in functionssh      
# file for logging any information Users dont need to specific this locat     
# ion as it is automatically calculate                                        
# @Type  :  Path
# @Final :  true
LOG_DIR=${LOCAL_PREFIX}/var/log/bigdata

UNIX_PATH_DIR=${LOCAL_PREFIX}/var/staging


###############################################################################
#                               Source Systems                                #
###############################################################################

###                                                                           
#                                                                             
# @Type  :  String
# @Final :  false
SOURCE_SYSTEM_SAMPLE=sample




###############################################################################
#                         PIG environment properties                          #
###############################################################################

###                                                                           
# Enable remote debugging for pig script                                      
# @Type  :  String
# @Final :  false
ENABLE_PIG_REMOTE_DEBUGGING=false


###                                                                           
# Enable remote debugging port for pig script                                 
# @Type  :  Integer
# @Final :  false
PIG_REMOTE_DEBUGGING_PORT=8000


###                                                                           
# Colon separated list of packages or Class names of UDFs                     
# @Type  :  String
# @Final :  false
ENABLE_PIG_INSTRUMENTATION=false


###                                                                           
# Colon separated list of packages or Class names of UDFs                     
# @Type  :  String
# @Final :  false
PIG_UDF_IMPORT_LIST=''




###############################################################################
#                      Repository environment properties                      #
###############################################################################

###                                                                           
# Relative path to a jar from repository folder which consists of pig ins     
# trumentation agent                                                          
# @Type  :  Path
# @Final :  false
PIG_INSTRUMENTATION_AGENT_JAR=com/datametica/governor/hadoop/pig/pig-instrumentor/1.0.0/pig-instrumentor-1.0.0.jar


###                                                                           
# Relative path to a jar from repository folder which consists of pig sta     
# ts publish and subscriber implementation                                    
# @Type  :  Path
# @Final :  false
PIG_STATS_JAR=com/datametica/governor/hadoop/pig/pig-stats/1.0.0/pig-stats-1.0.0.jar


###                                                                           
# Relative path to a jar from repository folder which consists of mapredu     
# ce sequence number generator job                                            
# @Type  :  Path
# @Final :  false
SEQUENCER_JAR=com/datametica/governor/hadoop/mapreduce/sequencer/1.0.0/sequencer-1.0.0.jar

###                                                                           
# Relative path to a jar from repository folder which consists of mapredu     
# ce xml generator generator job                                            
# @Type  :  Path
# @Final :  false
XML_GEN_JAR=com/datametica/governor/hadoop/mapreduce/xml-gen/1.0.0/xml-gen-1.0.0.jar

###                                                                           
# Relative path to a jar from shared folder which consists of md5 method  
# @Type  :  Path
# @Final :  false
PIG_UDF_BANK="/apps/appl/subject_areas/shared/shared-projects/shared-project/shared-project-modules/pig-udf-bank"
DATAFU_JAR=datafu-1.2.0.jar
UUID_JAR=uuid.jar
PIGGYBANK_JAR=piggybank-0.15.0.jar
UTIL_JAR=util.jar

###############################################################################
#                                Miscellaneous                                #
###############################################################################

###                                                                           
# Directory where all generated batch ids are stored This directory conta     
# ins a file called as current this file consists of the current batch id     
# which is being used by this batch Once batch is completed the current       
# file is renamed to the batch id it had stored This directory consists o     
# f all the batch id files from the begining till date                        
# @Type  :  Path
# @Final :  true
BATCH_ID_DIR=${LOCAL_PREFIX}/var/batch

###
# Directory where all generated run dates are stored This directory conta
# ins a file called as current this file consists of the current run date
# which is being used by this batch Once batch is completed the current
# file is renamed to the run date it had stored This directory consists of
# all the run date files from the begining till date
# @Type  :  Path
# @Final :  true
RUN_DATE_DIR=${LOCAL_PREFIX}/var/rundate

###                                                                           
# Id that should be used to store raw data when it lands into incoming or     
# security layer                                                              
# @Type  :  Number
# @Final :  true
LANDING_PARTITION_BATCH_ID=00000000000000


###                                                                           
# Fail the job if the expected intput for the job is not available            
# @Type  :  String
# @Final :  true
NO_INPUT_DATA_STRATEGY_FAIL=fail


###                                                                           
# Wait for a timeout defined by below definded property if the expected i     
# ntput for the job is not available                                          
# @Type  :  String
# @Final :  true
NO_INPUT_DATA_STRATEGY_WAIT=wait


###                                                                           
# Wait timout in seconds for above degined wait strategy Fail the job aft     
# er the timeout occurs                                                       
# @Type  :  Long
# @Final :  false
NO_INPUT_DATA_STRATEGY_WAIT_TIMEOUT_SECONDS=100000


###                                                                           
# Continue the job execution even though the input job is not available       
# @Type  :  String
# @Final :  false
NO_INPUT_DATA_STRATEGY_CONTINUE=continue


###                                                                           
# Strategy that should be used if the input for the job is not available      
# @Type  :  String
# @Final :  false
NO_INPUT_DATA_STRATEGY=${NO_INPUT_DATA_STRATEGY_WAIT}


###                                                                           
# Two character prefix that is used to prepend to all the properties defi     
# ned by any module to avoid any name conflicts with existing properties      
# defined by different frameworks used by the modules                         
# @Type  :  String
# @Final :  true
UNIQUE_PROPERTY_PREFIX=sl


###                                                                           
# Prefix that should be used to hide the files or folders from being proc     
# essed                                                                       
# @Type  :  String
# @Final :  true
HIDDEN_FILE_OR_DIRECTORY_PREFIX=_

###############################################################################
#                        MYSQL AUDIT CONTROL INFO                             #
###############################################################################

CTRL_DB_USERNAME=express
CTRL_DB_PASSWORD=
CTRL_DB_HOST=CMHLDDLKEEDG02.expdev.local
AUDIT_CTRL_DB=Express_Ingestion
AUDIT_CTRL_TBL_NAME=Express_Control

###############################################################################
#                        ORACLE AUDIT CONTROL INFO                             #
###############################################################################

ORA_CTRL_DB_USERNAME=NIFI_TRACKING
ORA_CTRL_DB_PASSWORD=qubRuch8
ORA_CTRL_DB_HOST=cmhcanlyodb
ORA_AUDIT_CTRL_TBL_NAME=Express_Control
ORA_CTRL_DB_SID=cmhcanly.expressco.com

###############################################################################
#                        PROSPECT AUDIT CONTROL INFO                           #
###############################################################################

PROSP_AUD_DB_USERNAME=NIFI_TRACKING
PROSP_AUD_DB_PASSWORD=qubRuch8
PROSP_AUD_DB_HOST=cmhcanlyodb
PROSP_AUD_DB_PORT=1521
PROSP_AUD_DB=cmhcanly.expressco.com

###############################################################################
#                        ALERT MAILER LIST                                    #
###############################################################################

alert_mailer_id=expithadoopsupportteam@express.com

##############################################################################
#                       KERBEROS KEYTAB INFO                                 #
##############################################################################

KEYTAB_FILE=/etc/security/keytabs/sa-custdatahub.keytab
TRUSTED_USER=sa-custdatahub@EXPRESSCO.COM


##############################################################################
#                       KERBEROS KEYTAB INFO                                 #
##############################################################################

KEYTAB_FILE=/etc/security/keytabs/sa-devcustdatahub.keytab
TRUSTED_USER=sa-devcustdatahub@EXPDEV.LOCAL


######################################
#Surrogate key validation properties #
######################################

MAX_DUPICATE_NATURAL_KEY_COUNT=0

 
######################################
#Nifi Process Group Id File          #
######################################

PROCESS_GROUP_ID_PROPERTY_FILE=/apps/appl/subject_areas/customer/customer_project/nifi_ingestion_workflow/etc/process_group_id.prod.properties 

######################################
#ECAT properties                     #
######################################

ECAT_URL=http://cmhlpdlakect01.expressco.com:8080
ECAT_USER=admin
ECAT_PWD=admin
ECAT_DS="\"CDH Prod\""

######################################
#Spark Common properties             #
######################################

SPARK_MASTER=yarn
SPARK_DEPLOY_MODE=cluster
COALESCE_PARTITIONS=100
SPARK_LIB_LOCATION=/usr/hdp/current/spark-client/lib
SPARK_EXTRA_JARS="${SPARK_LIB_LOCATION}/datanucleus-api-jdo-3.2.6.jar,${SPARK_LIB_LOCATION}/datanucleus-rdbms-3.2.9.jar,${SPARK_LIB_LOCATION}/datanucleus-core-3.2.10.jar,${HOME}/subject_areas/customer/customer_project/lib/ojdbc6.jar"
SPARK_EXTRA_FILES="${SPARK_LIB_LOCATION}/../conf/hive-site.xml,${HOME}/subject_areas/customer/etc/log4j.properties"
SPARK_ECAT_PROP="-Decat.url=${ECAT_URL} -Decat.username=${ECAT_USER} -Decat.password=${ECAT_PWD} -Decat.datastore=${ECAT_DS}"
SPARK_LIB_PATH="${SPARK_LIB_LOCATION}/spark-hdp-assembly.jar"


###############################################################################
#                                     End                                     #
###############################################################################


